# Integrating Knowledge into Distributed Agents Based on Petals

## Overview

Integrating knowledge into distributed agents based on language models is crucial for enhancing their efficiency in specialized domains. This repository explores methods of knowledge integration using the Petals library, which provides capabilities for inference and fine-tuning models on distributed machines. This approach allows for the effective integration of specialized knowledge while minimizing computational resources through Parameter-Efficient Fine-Tuning (PEFT) methods, such as P-Tuning (Prompt Tuning).

## Objectives

The main objective of this research is to compare various methods of knowledge integration and propose a new approach to enhance the efficiency of distributed agents. The study focuses on:

- Distributed agents based on language models.
- Methods of integrating domain-specific knowledge to improve their efficiency.

## Research Methods

The methods used in this research include:

- Machine learning
- Natural language processing
- Fine-tuning generative models
- Retrieval-Augmented Generation (RAG) methods
- Specific approaches to domain knowledge integration

## Approach

This repository uses the Petals library for distributed inference and fine-tuning. The research specifically explores Parameter-Efficient Fine-Tuning (PEFT) methods, such as P-Tuning (Prompt Tuning), and compares their effectiveness with Retrieval-Augmented Generation (RAG) methods.

### Key Components

1. **Petals Library**: Used for inference and fine-tuning models on distributed machines.
2. **Parameter-Efficient Fine-Tuning (PEFT)**: Focuses on P-Tuning (Prompt Tuning) for effective knowledge integration.
3. **Retrieval-Augmented Generation (RAG)**: Methods for contextually enhancing language model outputs.

## Results

The results of this study show that:

- Prompt-Tuning (PT) is more effective than Retrieval-Augmented Generation (RAG).
- Models fine-tuned with PT demonstrate higher performance.
- Combining PT with RAG produces a synergistic effect, further improving performance.

## Conclusion

This study confirms that knowledge integration methods using Prompt-Tuning significantly enhance the performance of distributed agents. This opens up new possibilities for their application in various domains, such as finance, improving the accuracy and efficiency of task execution.
